{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ee0855-c539-4114-9121-aa55c2740e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a stripped down version of the original code that doesn't involve dashboards\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gliner_spacy.pipeline import GlinerSpacy\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be6d1c3-1e30-44ce-b2d0-0fc01d04e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gliner_spacy.pipeline import GlinerSpacy\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "## Load Google's content categories\n",
    "def load_google_categories(url='https://www.google.com/basepages/producttype/taxonomy-with-ids.en-US.txt'):\n",
    "    f = requests.get(url).text\n",
    "    google_categories = f.split('\\n')\n",
    "    google_categories = [x.split(' - ') for x in google_categories]\n",
    "    google_categories = {x[1].strip():int(x[0].strip()) for x in google_categories if len(x)==2}\n",
    "    return google_categories\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The sentencepiece tokenizer\")\n",
    "\n",
    "# Configuration for GLiNER integration\n",
    "custom_spacy_config = {\n",
    "    \"gliner_model\": \"urchade/gliner_small-v2.1\",\n",
    "    \"chunk_size\": 128,\n",
    "    \"labels\": [\"person\", \"organization\", \"location\", \"event\", \"work_of_art\", \"product\", \"service\", \"date\", \"number\", \"price\", \"address\", \"phone_number\", \"misc\"],\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "\n",
    "# Model variables for lazy loading\n",
    "nlp = None\n",
    "sentence_model = None\n",
    "google_categories = []\n",
    "\n",
    "# Function to lazy load NLP model\n",
    "def get_nlp():\n",
    "    global nlp\n",
    "    if nlp is None:\n",
    "        try:\n",
    "            logger.info(\"Loading spaCy model\")\n",
    "            nlp = spacy.blank(\"en\")\n",
    "            nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)\n",
    "            logger.info(\"spaCy model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error loading spaCy model\")\n",
    "            raise\n",
    "    return nlp\n",
    "\n",
    "# Function to lazy load sentence transformer model\n",
    "def get_sentence_model():\n",
    "    global sentence_model\n",
    "    if sentence_model is None:\n",
    "        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return sentence_model\n",
    "\n",
    "# Function to precompute category embeddings\n",
    "def compute_category_embeddings():\n",
    "    try:\n",
    "        categories = list(load_google_categories().keys())\n",
    "        return get_sentence_model().encode(categories)\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "# Function to perform topic modeling using sentence transformers\n",
    "def perform_topic_modeling_from_similarities(similarities):\n",
    "    try:\n",
    "        categories = list(load_google_categories().keys())\n",
    "        top_indices = similarities.argsort()[-3:][::-1]\n",
    "        \n",
    "        best_match = categories[top_indices[0]]\n",
    "        second_best = categories[top_indices[1]]\n",
    "        \n",
    "        if similarities[top_indices[0]] > similarities[top_indices[1]] * 1.1:\n",
    "            return best_match\n",
    "        else:\n",
    "            return f\"{best_match} , {second_best}\"\n",
    "    except Exception as e:\n",
    "        return \"Error in topic modeling\"\n",
    "\n",
    "## Optimized batch processing of keywords\n",
    "def batch_process_keywords(keywords, batch_size=8):\n",
    "    processed_data = {'Keywords': [], 'Google Content Topics': [], 'Google Content Topic IDs': []}\n",
    "    \n",
    "    try:\n",
    "        sentence_model = get_sentence_model()\n",
    "        category_embeddings = compute_category_embeddings()\n",
    "        \n",
    "        for i in range(0, len(keywords), batch_size):\n",
    "            logger.info(f\"Processing {len(keywords)} keywords\")\n",
    "            batch = keywords[i:i+batch_size]\n",
    "            logger.info(f\"Processing batch {i//batch_size + 1}\")\n",
    "            batch_embeddings = sentence_model.encode(batch, batch_size=batch_size, show_progress_bar=False)\n",
    "                        \n",
    "            similarities = cosine_similarity(batch_embeddings, category_embeddings)\n",
    "            Google_Content_Topics = [perform_topic_modeling_from_similarities(sim) for sim in similarities]\n",
    "            cats = [x.split(' , ') for x in Google_Content_Topics]\n",
    "            cats = [[category_with_ids[x] for x in item] for item in cats]\n",
    "            \n",
    "            processed_data['Keywords'].extend(batch)\n",
    "            processed_data['Google Content Topics'].extend(Google_Content_Topics)\n",
    "            processed_data['Google Content Topic IDs'].extend(cats)\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "        logger.info(\"Keyword processing completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"An error occurred in batch_process_keywords\")\n",
    "    \n",
    "    return processed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8eb35ee-c7ae-4770-a9af-48f50256316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|████████████████████████████████| 175/175 [00:04<00:00, 36.03it/s]\n",
      "INFO:__main__:Processing 10 keywords\n",
      "INFO:__main__:Processing batch 1\n",
      "INFO:__main__:Processing 10 keywords\n",
      "INFO:__main__:Processing batch 2\n",
      "INFO:__main__:Keyword processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "category_with_ids = load_google_categories()\n",
    "products = []\n",
    "with open('./example.txt','r') as f:\n",
    "    for line in f:\n",
    "        products.append(line.strip())\n",
    "results = batch_process_keywords(products)\n",
    "pd.DataFrame(results).to_csv('./temp.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019d030-6e5d-4e5e-8c2f-23f6d26ce409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0eb9d7-c30e-4ed6-a0cd-54f6d79415a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvkeyintent)",
   "language": "python",
   "name": "venvkeyintent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
